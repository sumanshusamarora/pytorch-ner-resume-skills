{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchnlp.samplers import DistributedBatchSampler, BalancedSampler\n",
    "from torchnlp.datasets.dataset import Dataset\n",
    "import torchnlp\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "from torchnlp.encoders.text import StaticTokenizerEncoder, CharacterEncoder\n",
    "from torchcrf import CRF\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json('../data/Entity Recognition in Resumes.json', lines=True)\n",
    "\n",
    "cv_text = np.array(dataset.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(inp):\n",
    "    def clean(txt):\n",
    "        return \"\\n\".join(line.lower().replace('â€¢', '').replace('-', '') for line in\n",
    "                  txt.split('\\n'))\n",
    "    if isinstance(inp, list):\n",
    "        return_out = \",\".join([clean(string) for string in inp])\n",
    "    elif isinstance(inp, str):\n",
    "        return_out = clean(inp)\n",
    "\n",
    "    return return_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "for ind, annotation in enumerate(dataset.annotation):\n",
    "    _ = [all_labels.append(entity_lst['label']) for entity_lst in annotation if entity_lst['label'] not in all_labels and len(entity_lst['label']) > 0]\n",
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_reformatted = pd.DataFrame(columns=[\"documentNum\", \"documentText\", \"documentAnnotation\", \"sentenceNum\", \"sentence\", \"labelsDict\", \"containsLabel\", \"wordNum\", \"word\", \"labelName\"])\n",
    "k = 0\n",
    "for i in range(len(dataset)):\n",
    "    dataset_reformatted.loc[k, \"documentNum\"] = i+1\n",
    "    dataset_reformatted.loc[k, \"documentText\"] = dataset.content[i]\n",
    "    dataset_reformatted.loc[k, \"documentAnnotation\"] = dataset.annotation[i]\n",
    "    skill_label = [cv_label['label'][0] for cv_label in dataset.annotation[i]  if len(cv_label['label']) > 0]\n",
    "    skills =  [re.split(',\\n', clean_text(cv_label['points'][0]['text'])) for cv_label in dataset.annotation[i]  if len(cv_label['label']) > 0]\n",
    "    skills_dict = dict(zip(skill_label, skills))\n",
    "    dataset_reformatted.loc[k, \"labelsDict\"] = json.dumps(skills_dict)\n",
    "    \n",
    "    for sent_num, sent in enumerate(clean_text(dataset.content[i]).split(\"\\n\")):\n",
    "        if sent.strip() != \"\":\n",
    "            dataset_reformatted.loc[k, \"sentenceNum\"] = f\"{i+1}.{sent_num+1}\"\n",
    "            dataset_reformatted.loc[k, \"sentence\"] = sent\n",
    "\n",
    "            found_skills = dict()\n",
    "            contains_labels = 0\n",
    "            for it, val in skills_dict.items():\n",
    "                ss = [in_val for in_val in val if in_val in sent]\n",
    "                found_skills[it] = ss\n",
    "                if len(ss) > 0:\n",
    "                    contains_labels = 1\n",
    "\n",
    "            dataset_reformatted.loc[k, \"containsLabel\"] = contains_labels\n",
    "\n",
    "            for w, word in enumerate(sent.replace(',', '').split(' ')):\n",
    "                dataset_reformatted.loc[k, \"wordNum\"] = f\"{i+1}.{sent_num+1}.{w+1}\"\n",
    "                dataset_reformatted.loc[k, \"word\"] = word\n",
    "\n",
    "                if contains_labels:\n",
    "                    for it, val in found_skills.items():\n",
    "                        if word in \" \".join(val) and \" \".join(val).strip() != \"\" and word.strip() != \"\":\n",
    "                            dataset_reformatted.loc[k, \"labelName\"] = it\n",
    "\n",
    "                k+=1\n",
    "                \n",
    "dataset_reformatted.fillna('', inplace=True)\n",
    "dataset_reformatted.loc[dataset_reformatted.labelName=='', 'labelName'] = 'O'\n",
    "dataset_reformatted.to_pickle(\"../data/dataset_reformatted.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(dataset_reformatted.sentenceNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_list = []\n",
    "y_binary_list = []\n",
    "y_ner_list = []\n",
    "for sentenceNum in pd.unique(dataset_reformatted.sentenceNum):\n",
    "    if sentenceNum != \"\":\n",
    "        sent_data = dataset_reformatted[dataset_reformatted.wordNum.str.startswith(f'{sentenceNum}.')]\n",
    "        X_text_list.append(list(sent_data.word))\n",
    "        y_ner_list.append(list(sent_data.labelName))\n",
    "        if len([label for label in sent_data.labelName if label != 'O']) > 0:\n",
    "            y_binary_list.append(1)\n",
    "        else:\n",
    "            y_binary_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ready = pd.DataFrame(data={'X_text':X_text_list, 'y_binary':y_binary_list, 'y_ner':y_ner_list})\n",
    "dataset_ready.to_pickle(\"../data/dataset_ready.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run below cells if you dont wish to preprocess the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ready = pd.read_pickle(\"../data/dataset_ready.pkl\")\n",
    "X_text_list = list(dataset_ready.X_text)\n",
    "y_binary_list = list(dataset_ready.y_binary)\n",
    "y_ner_list = list(dataset_ready.y_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tags = []\n",
    "for lst in X_text_list:\n",
    "    postag = nltk.pos_tag([word if word.strip() != \"\" else '<OOS>' for word in lst])\n",
    "    X_tags.append([tag[1] for tag in postag])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_tags_unq_list = []\n",
    "\n",
    "_ = [[X_tags_unq_list.append(tag) for tag in lst if tag not in X_tags_unq_list] for lst in X_tags]\n",
    "X_tags_map = {tag:i+1 for i,tag in enumerate(X_tags_unq_list)}\n",
    "\n",
    "X_tags_encoded = []\n",
    "for lst in X_tags:\n",
    "    postag = []\n",
    "    X_tags_encoded.append([X_tags_map[tag] for tag in lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = random.choices(range(len(X_text_list)), k=int(.3*len(X_text_list)))\n",
    "train_index = [ind for ind in range(len(X_text_list)) if ind not in test_index]\n",
    "\n",
    "X_text_list_train = [X_text_list[ind] for ind in train_index]\n",
    "X_text_list_test = [X_text_list[ind] for ind in test_index]\n",
    "\n",
    "X_tags_train = [X_tags[ind] for ind in train_index]\n",
    "X_tags_test = [X_tags[ind] for ind in test_index]\n",
    "\n",
    "y_binary_list_train = [y_binary_list[ind] for ind in train_index]\n",
    "y_binary_list_test = [y_binary_list[ind] for ind in test_index]\n",
    "\n",
    "y_ner_list_train = [y_ner_list[ind] for ind in train_index]\n",
    "y_ner_list_test = [y_ner_list[ind] for ind in test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LEN = max([len(sentence) for sentence in X_text_list_train])\n",
    "ALL_LABELS = []\n",
    "_ = [[ALL_LABELS.append(label) for label in lst] for lst in y_ner_list_train]\n",
    "CLASS_COUNT_OUT = np.unique(ALL_LABELS, return_counts=True)\n",
    "CLASS_COUNT_DICT = dict(zip(CLASS_COUNT_OUT[0], CLASS_COUNT_OUT[1]))\n",
    "NUM_CLASSES = len([clas for clas in CLASS_COUNT_DICT.keys()])\n",
    "print(F\"Max sentence length - {MAX_SENTENCE_LEN}, Total Classes = {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoder=StaticTokenizerEncoder(sample=X_text_list_train, \n",
    "                                 append_eos=False, \n",
    "                                 tokenize=lambda x: x,\n",
    "                                 )\n",
    "x_encoded_train = [x_encoder.encode(text) for text in X_text_list_train]\n",
    "x_padded_train = torch.LongTensor(pad_sequence(x_encoded_train, MAX_SENTENCE_LEN+1))\n",
    "\n",
    "x_encoded_test = [x_encoder.encode(text) for text in X_text_list_test]\n",
    "x_padded_test = torch.LongTensor(pad_sequence(x_encoded_test, MAX_SENTENCE_LEN+1))\n",
    "\n",
    "if x_padded_train.shape[1] > x_padded_test.shape[1]:\n",
    "    x_padded_test = torch.cat((x_padded_test, torch.zeros(x_padded_test.shape[0], x_padded_train.shape[1]-x_padded_test.shape[1])), dim=1).type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_char_encoder=CharacterEncoder(sample=X_text_list_train, \n",
    "                                 append_eos=False, \n",
    "                                 )\n",
    "x_char_encoded_train = [[x_char_encoder.encode(x_encoder.index_to_token[word.item()]) for word in text] for text in x_padded_train]\n",
    "MAX_WORD_LENGTH = max([max([internal.shape[0] for internal in external]) for external in x_char_encoded_train])\n",
    "#x_char_padded = max([max([internal.shape[0] for internal in external]) for external in x_char_encoded])\n",
    "#x_char_padded = torch.LongTensor(pad_sequence(x_char_encoded, MAX_SENTENCE_LEN+1))\n",
    "outer_list = []\n",
    "for lst in x_char_encoded_train:\n",
    "    inner_list = []\n",
    "    for ten in lst:\n",
    "        res = torch.zeros(MAX_WORD_LENGTH, dtype=torch.long)\n",
    "        res[:ten.shape[0]] = ten\n",
    "        inner_list.append(res)\n",
    "    outer_list.append(inner_list)\n",
    "    \n",
    "x_char_padded_train = torch.stack([torch.stack(lst) for lst in outer_list])\n",
    "\n",
    "\n",
    "x_char_encoded_test = [[x_char_encoder.encode(x_encoder.index_to_token[word]) for word in text] for text in x_padded_test]\n",
    "outer_list = []\n",
    "for lst in x_char_encoded_test:\n",
    "    inner_list = []\n",
    "    for ten in lst:\n",
    "        res = torch.zeros(MAX_WORD_LENGTH, dtype=torch.long)\n",
    "        res[:ten.shape[0]] = ten\n",
    "        inner_list.append(res)\n",
    "    outer_list.append(inner_list)\n",
    "    \n",
    "x_char_padded_test = torch.stack([torch.stack(lst) for lst in outer_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_postag_encoder=StaticTokenizerEncoder(sample=X_tags_train, \n",
    "                                 append_eos=False, \n",
    "                                 tokenize=lambda x: x,\n",
    "                                 )\n",
    "x_postag_encoded_train = [x_postag_encoder.encode(text) for text in X_tags_train]\n",
    "x_postag_padded_train = torch.LongTensor(pad_sequence(x_postag_encoded_train, MAX_SENTENCE_LEN+1))\n",
    "#x_postag_ohe_train = torch.nn.functional.one_hot(x_postag_padded_train)\n",
    "\n",
    "x_postag_encoded_test = [x_postag_encoder.encode(text) for text in X_tags_test]\n",
    "x_postag_padded_test = torch.LongTensor(pad_sequence(x_postag_encoded_test, MAX_SENTENCE_LEN+1))\n",
    "\n",
    "if x_postag_padded_train.shape[1] > x_postag_padded_test.shape[1]:\n",
    "    x_postag_padded_test = torch.cat((x_postag_padded_test, torch.zeros(x_postag_padded_test.shape[0], x_postag_padded_train.shape[1]-x_postag_padded_test.shape[1])), dim=1).type(torch.long)\n",
    "\n",
    "#x_postag_ohe_test = torch.nn.functional.one_hot(x_postag_padded_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Ignore for now\n",
    "x_postag_encoded_train = [torch.LongTensor(tens) for tens in X_tags_train]\n",
    "x_postag_padded_train = torch.LongTensor(pad_sequence(x_postag_encoded_train, batch_first=True))\n",
    "x_postag_padded_train = torch.nn.functional.one_hot(x_postag_padded_train)\n",
    "\n",
    "x_postag_encoded_test = [torch.LongTensor(tens) for tens in X_tags_test]\n",
    "x_postag_padded_test = torch.LongTensor(pad_sequence(x_postag_encoded_test, batch_first=True))\n",
    "x_postag_padded_test = torch.nn.functional.one_hot(x_postag_padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ner_encoder = LabelEncoder(sample=CLASS_COUNT_DICT.keys())\n",
    "y_ner_encoded_train = [[y_ner_encoder.encode(label) for label in label_list] for label_list in y_ner_list_train]\n",
    "y_ner_encoded_train = [torch.stack(tens) for tens in y_ner_encoded_train]\n",
    "y_ner_padded_train = torch.LongTensor(pad_sequence(y_ner_encoded_train, MAX_SENTENCE_LEN+1))\n",
    "\n",
    "y_ner_encoded_test = [[y_ner_encoder.encode(label) for label in label_list] for label_list in y_ner_list_test]\n",
    "y_ner_encoded_test = [torch.stack(tens) for tens in y_ner_encoded_test]\n",
    "y_ner_padded_test = torch.LongTensor(pad_sequence(y_ner_encoded_test, MAX_SENTENCE_LEN+1))\n",
    "\n",
    "if y_ner_padded_train.shape[1] > y_ner_padded_test.shape[1]:\n",
    "    y_ner_padded_test = torch.cat((y_ner_padded_test, torch.zeros(y_ner_padded_test.shape[0], y_ner_padded_train.shape[1]-y_ner_padded_test.shape[1])), dim=1).type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ner_encoder.index_to_token[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary_series_train = pd.Series(y_binary_list_train).astype(\"category\")\n",
    "y_binary_tensor_train = torch.LongTensor(y_binary_series_train).unsqueeze(1)\n",
    "y_binary_encoded_train = torch.LongTensor(y_binary_series_train)\n",
    "\n",
    "y_binary_series_test = pd.Series(y_binary_list_test).astype(\"category\")\n",
    "y_binary_tensor_test = torch.LongTensor(y_binary_series_test).unsqueeze(1)\n",
    "y_binary_encoded_test = torch.LongTensor(y_binary_series_test)\n",
    "\n",
    "y_binary_encode_map = {code:cat for code, cat in enumerate(y_binary_series_train.cat.categories)}\n",
    "y_binary_encode_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityExtraction(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes, word_embed_dim=124, tag_embed_dim=124, char_embed_dim=124, rnn_embed_dim=512, \n",
    "                 char_embedding=True, dropout_ratio = 0.3):\n",
    "        super().__init__()\n",
    "        #self variables\n",
    "        self.NUM_CLASSES = num_classes\n",
    "        self.word_embed_dim = word_embed_dim\n",
    "        self.tag_embed_dim = tag_embed_dim\n",
    "        self.char_embed_dim = char_embed_dim\n",
    "        self.rnn_embed_dim = rnn_embed_dim\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        \n",
    "        #Embedding Layers\n",
    "        self.word_embed = nn.Embedding(num_embeddings=x_encoder.vocab_size, \n",
    "                                       embedding_dim=self.word_embed_dim)\n",
    "        self.word_embed_drop = nn.Dropout(self.dropout_ratio)\n",
    "        \n",
    "        self.char_embed = nn.Embedding(num_embeddings=x_char_encoder.vocab_size, \n",
    "                                       embedding_dim=self.char_embed_dim)\n",
    "        self.char_embed_drop = nn.Dropout(self.dropout_ratio)\n",
    "        \n",
    "        self.postag_embed = nn.Embedding(num_embeddings=x_postag_encoder.vocab_size, \n",
    "                                         embedding_dim=self.tag_embed_dim)\n",
    "        self.tag_embed_drop = nn.Dropout(self.dropout_ratio)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #CNN for character input\n",
    "        self.conv_char = nn.Conv1d(in_channels=self.char_embed_dim, out_channels=52, kernel_size=3, padding=1)\n",
    "        #self.maxpool_char = nn.MaxPool1d(kernel_size=3)\n",
    "        \n",
    "        #LSTM for concatenated input\n",
    "        self.lstm_binary = nn.LSTM(input_size=5760, \n",
    "                             hidden_size=512,\n",
    "                             num_layers=2,\n",
    "                             batch_first=True,\n",
    "                             dropout=0.3,\n",
    "                             bidirectional=True)\n",
    "        self.lstm_binary_drop = nn.Dropout(self.dropout_ratio)\n",
    "        \n",
    "        self.lstm_ner = nn.LSTM(input_size=5761, \n",
    "                             hidden_size=512,\n",
    "                             num_layers=2,\n",
    "                             batch_first=True,\n",
    "                             dropout=0.3,\n",
    "                             bidirectional=True)\n",
    "        self.lstm_ner_drop = nn.Dropout(self.dropout_ratio)\n",
    "        \n",
    "        #Linear layer\n",
    "        self.linear_binary = nn.Linear(in_features=1024, out_features=1)\n",
    "        \n",
    "        #self.linear1 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.linear_ner = nn.Linear(in_features=1024, out_features=self.NUM_CLASSES+1) #+1 for padding 0\n",
    "\n",
    "        \n",
    "    def forward(self, x_word, x_char, x_pos, y_binary=None):\n",
    "        x_char_shape = x_char.shape\n",
    "        batch_size = x_char_shape[0]\n",
    "        \n",
    "        word_out = self.word_embed(x_word)\n",
    "        word_out = self.word_embed_drop(word_out)\n",
    "        \n",
    "        char_out = self.char_embed(x_char)\n",
    "        char_out = self.char_embed_drop(char_out)\n",
    "        \n",
    "        tag_out = self.postag_embed(x_pos)\n",
    "        tag_out = self.tag_embed_drop(tag_out)\n",
    "        \n",
    "        char_out_shape = char_out.shape\n",
    "        char_out = char_out.view(char_out_shape[0], char_out_shape[1]*char_out_shape[2], char_out_shape[3])\n",
    "        char_out = self.conv_char(char_out.permute(0,2,1))\n",
    "        char_out = char_out.view(char_out_shape[0], char_out_shape[1], -1)\n",
    "\n",
    "        \n",
    "        concat = torch.cat((word_out, char_out, tag_out), dim=2)\n",
    "        concat = F.relu(concat)\n",
    "        \n",
    "        \n",
    "        \n",
    "        lstm_out, (h, c) = self.lstm_binary(concat)\n",
    "        lstm_out = self.lstm_binary_drop(lstm_out)\n",
    "        \n",
    "        #Binary model result\n",
    "        binary_out = self.linear_binary(lstm_out[:,-1,:])\n",
    "        binary_out = torch.sigmoid(binary_out)\n",
    "        \n",
    "        if y_binary is None:\n",
    "            replicate = binary_out\n",
    "        else:\n",
    "            replicate = y_binary\n",
    "     \n",
    "        replicate_repeat = replicate.repeat_interleave(concat.shape[1],0).view(batch_size,concat.shape[1],-1)\n",
    "        \n",
    "        \n",
    "        #Concatenate binary result with embeddings\n",
    "        concat = torch.cat((concat, replicate_repeat), dim=2)\n",
    "        \n",
    "        #NER LSTM\n",
    "        ner_lstm_out, _ = self.lstm_ner(concat)\n",
    "        ner_lstm_out = self.lstm_ner_drop(ner_lstm_out)\n",
    "        \n",
    "        ner_out = self.linear_ner(ner_lstm_out)\n",
    "        \n",
    "        return binary_out, ner_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityextractor = EntityExtraction(num_classes=NUM_CLASSES)\n",
    "torch.cuda.empty_cache()\n",
    "entityextractor = entityextractor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_class_weights = compute_class_weight('balanced', classes=[0,1], y=y_binary_encoded_train.numpy())\n",
    "binary_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_class_weights = compute_class_weight('balanced', classes=np.unique(torch.flatten(y_ner_padded_train).numpy()), y=torch.flatten(y_ner_padded_train).numpy())\n",
    "ner_class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_crossentropy = nn.CrossEntropyLoss(weight=torch.FloatTensor(ner_class_weights).to(device))\n",
    "optimizer = torch.optim.Adam(entityextractor.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_out, ner_out = entityextractor(x_padded_train[0:2].to(device), \n",
    "                                      x_char_padded_train[0:2].to(device), \n",
    "                                      x_postag_padded_train[0:2].to(device), \n",
    "                                      torch.LongTensor(y_binary_tensor_train[0:2]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset([{'x_padded':x_padded_train[i],\n",
    "                    'x_char_padded':x_char_padded_train[i],\n",
    "                    'x_postag_padded':x_postag_padded_train[i],\n",
    "                    'y_binary_tensor':y_binary_tensor_train[i],\n",
    "                    'y_ner_padded':y_ner_padded_train[i],\n",
    "                    'y_binary_encoded':y_binary_encoded_train[i]} for i in range(x_padded_train.shape[0])])\n",
    "\n",
    "dataloader_train=DataLoader(dataset=dataset_train, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = Dataset([{'x_padded':x_padded_test[i],\n",
    "                    'x_char_padded':x_char_padded_test[i],\n",
    "                    'x_postag_padded':x_postag_padded_test[i],\n",
    "                    'y_binary_tensor':y_binary_tensor_test[i],\n",
    "                    'y_ner_padded':y_ner_padded_test[i],\n",
    "                    'y_binary_encoded':y_binary_encoded_test[i]} for i in range(x_padded_test.shape[0])])\n",
    "\n",
    "dataloader_test=DataLoader(dataset=dataset_test, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(epoch_losses, test_epoch_loss, epoch_accuracy, test_epoch_accuracy, epoch_precision, \n",
    "         test_epoch_precision, epoch_recall, test_epoch_recall, epoch_f1s, test_epoch_f1s):\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,10))\n",
    "    ax = fig.add_subplot(2,3,1)\n",
    "    ax.plot(epoch_losses, color='b', label=\"Train\")\n",
    "    ax.plot(test_epoch_loss, color='g', label=\"Test\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Losses\")\n",
    "\n",
    "    ax = fig.add_subplot(2,3,2)\n",
    "    ax.plot(epoch_accuracy, color='b', label=\"Train\")\n",
    "    ax.plot(test_epoch_accuracy, color='g', label=\"Test\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Accuracy\")\n",
    "\n",
    "    ax = fig.add_subplot(2,3,3)\n",
    "    ax.plot(epoch_precision, color='b', label=\"Train\")\n",
    "    ax.plot(test_epoch_precision, color='g', label=\"Test\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Precision\")\n",
    "\n",
    "    ax = fig.add_subplot(2,3,4)\n",
    "    ax.plot(epoch_recall, color='b', label=\"Train\")\n",
    "    ax.plot(test_epoch_recall, color='g', label=\"Test\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"Recall\")\n",
    "\n",
    "    ax = fig.add_subplot(2,3,6)\n",
    "    ax.plot(epoch_f1s, color='b', label=\"Train\")\n",
    "    ax.plot(test_epoch_f1s, color='g', label=\"Test\")\n",
    "    ax.legend()\n",
    "    ax.set_title(\"F1\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags = NUM_CLASSES+1\n",
    "crf_model = CRF(num_tags).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "epoch_losses = []\n",
    "epoch_accuracy = []\n",
    "epoch_recall = []\n",
    "epoch_precision = []\n",
    "epoch_f1s = []\n",
    "\n",
    "\"\"\n",
    "test_epoch_loss = []\n",
    "test_epoch_accuracy = []\n",
    "test_epoch_recall = []\n",
    "test_epoch_precision = []\n",
    "test_epoch_f1s = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_metric_append = int(len(dataloader_train)/4)\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n\\n------------------------- Epoch - {epoch+1} -------------------------\")\n",
    "    batch_losses = []\n",
    "    batch_accuracy = []\n",
    "    batch_f1s = []\n",
    "    batch_recalls = []\n",
    "    batch_precisions = []\n",
    "    total_corrects = 0\n",
    "    total_predicted = 0\n",
    "    binary_batch_all = None\n",
    "    binary_out_result_all = None\n",
    "    \n",
    "    test_binary_batch_all = None\n",
    "    test_binary_out_result_all = None\n",
    "    \n",
    "    for batch_num, data in enumerate(dataloader_train):\n",
    "        \n",
    "        weight_this_batch = torch.FloatTensor([binary_class_weights[val] for val in data['y_binary_encoded'].numpy()]).to(device).unsqueeze(1)\n",
    "        criterion_binary = nn.BCELoss(weight=weight_this_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        data['x_padded'] = data['x_padded'].to(device)\n",
    "        data['x_char_padded'] = data['x_char_padded'].to(device)\n",
    "        data['x_postag_padded'] = data['x_postag_padded'].to(device)\n",
    "        data['y_binary_tensor'] = data['y_binary_tensor'].to(device)\n",
    "        data['y_binary_encoded'] = data['y_binary_encoded'].type(torch.float).unsqueeze(1).to(device)\n",
    "        data['y_ner_padded'] = data['y_ner_padded'].to(device)\n",
    "        \n",
    "        binary_out, ner_out = entityextractor(data['x_padded'], \n",
    "                                              data['x_char_padded'],\n",
    "                                              data['x_postag_padded'],\n",
    "                                              data['y_binary_tensor'])\n",
    "        \n",
    "        binary_loss = criterion_binary(binary_out, data['y_binary_encoded'])\n",
    "        #ner_loss = criterion_crossentropy(ner_out.transpose(2,1), data['y_ner_padded'])\n",
    "        ner_loss = crf_model(ner_out.permute(1,0,2), data['y_ner_padded'].permute(1,0))\n",
    "\n",
    "        #Loss \n",
    "        loss = torch.mean(torch.stack((binary_loss, ner_loss)))\n",
    "        batch_losses.append(loss.item())\n",
    "        \n",
    "        #Evaluation Metrics\n",
    "        binary_out_result = (binary_out >= 0.5).type(torch.int).squeeze(-1)\n",
    "        binary_out_result = np.array([y_binary_encode_map[v.item()] for v in binary_out_result])\n",
    "        binary_batch = data['y_binary_encoded'].to('cpu').type(torch.int).numpy()\n",
    "        \n",
    "        if not binary_batch_all is None:\n",
    "            binary_batch_all = np.append(binary_batch_all, binary_batch)\n",
    "            binary_out_result_all = np.append(binary_out_result_all, binary_out_result)\n",
    "        else:\n",
    "            binary_batch_all = binary_batch.copy()\n",
    "            binary_out_result_all = binary_out_result.copy()\n",
    "        \n",
    "    \n",
    "        precision = precision_score(binary_batch_all, binary_out_result_all)\n",
    "        accuracy = accuracy_score(binary_batch_all, binary_out_result_all)\n",
    "        f1 = f1_score(binary_batch_all, binary_out_result_all)\n",
    "        recall = recall_score(binary_batch_all, binary_out_result_all)\n",
    "        \n",
    "        batch_f1s.append(f1)\n",
    "        batch_accuracy.append(accuracy)\n",
    "        batch_recalls.append(recall)\n",
    "        batch_precisions.append(precision)\n",
    "        \n",
    "        \n",
    "        if batch_num%index_metric_append == 0 and batch_num != 0:\n",
    "            print(f\"--> Batch - {batch_num+1}, \" +\n",
    "                  f\"Loss - {np.array(batch_losses).mean():.4f} (B-{binary_loss.item():.4f}, N-{ner_loss.item():.4f}), \"+\n",
    "                  f\"Accuracy - {accuracy:.2f}, \"+\n",
    "                  f\"Recall - {recall:.2f}, \"+\n",
    "                  f\"Precision - {precision:.2f}, \"+\n",
    "                  f\"F1 - {f1:.2f}\")\n",
    "        \n",
    "        break\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    break\n",
    "    epoch_losses.append(np.array(batch_losses).mean())\n",
    "    epoch_accuracy.append(np.array(batch_accuracy).mean())\n",
    "    epoch_recall.append(np.array(batch_recalls).mean())\n",
    "    epoch_precision.append(np.array(batch_precisions).mean())\n",
    "    epoch_f1s.append(np.array(batch_f1s).mean())\n",
    "    \n",
    "    \n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    test_precisions = []\n",
    "    test_recalls = []\n",
    "    test_f1s = []\n",
    "    \n",
    "    print(\"************Evaluating validation data now***************\")\n",
    "    for k, data_test in enumerate(dataloader_test):\n",
    "        with torch.no_grad():\n",
    "            test_weight_this_batch = torch.FloatTensor([binary_class_weights[val] for val in data_test['y_binary_encoded'].numpy()]).to(device).unsqueeze(1)\n",
    "            test_criterion_binary = nn.BCELoss(weight=test_weight_this_batch)\n",
    "            \n",
    "            data_test['x_padded'] = data_test['x_padded'].to(device)\n",
    "            data_test['x_char_padded'] = data_test['x_char_padded'].to(device)\n",
    "            data_test['x_postag_padded'] = data_test['x_postag_padded'].to(device)\n",
    "            data_test['y_binary_encoded'] = data_test['y_binary_encoded'].type(torch.float).unsqueeze(1).to(device)\n",
    "            data_test['y_ner_padded'] = data_test['y_ner_padded'].to(device)\n",
    "        \n",
    "            test_binary_out, test_ner_out = entityextractor(data_test['x_padded'], data_test['x_char_padded'], data_test['x_postag_padded'])\n",
    "            \n",
    "            \n",
    "            test_binary_loss = test_criterion_binary(test_binary_out, data_test['y_binary_encoded'])\n",
    "            test_ner_loss = criterion_crossentropy(test_ner_out.transpose(2,1), data_test['y_ner_padded'])\n",
    "\n",
    "            #Loss \n",
    "            test_loss = torch.mean(torch.stack((test_binary_loss, test_ner_loss)))\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "            #Evaluation Metrics\n",
    "            test_binary_out_result = (test_binary_out >= 0.5).type(torch.int).squeeze(-1)\n",
    "            test_binary_out_result = np.array([y_binary_encode_map[v.item()] for v in test_binary_out_result])\n",
    "            test_binary_batch = data_test['y_binary_encoded'].to('cpu').type(torch.int).numpy()\n",
    "            \n",
    "            \n",
    "            if not test_binary_batch_all is None:\n",
    "                test_binary_batch_all = np.append(test_binary_batch_all, test_binary_batch)\n",
    "                test_binary_out_result_all = np.append(test_binary_out_result_all, test_binary_out_result)\n",
    "            else:\n",
    "                test_binary_batch_all = test_binary_batch.copy()\n",
    "                test_binary_out_result_all = test_binary_out_result.copy()\n",
    "\n",
    "            test_precision = precision_score(test_binary_batch_all, test_binary_out_result_all)\n",
    "            test_accuracy = accuracy_score(test_binary_batch_all, test_binary_out_result_all)\n",
    "            test_f1 = f1_score(test_binary_batch_all, test_binary_out_result_all)\n",
    "            test_recall = recall_score(test_binary_batch_all, test_binary_out_result_all)\n",
    "            \n",
    "            test_accs.append(test_accuracy)\n",
    "            test_precisions.append(test_precision)\n",
    "            test_recalls.append(test_f1)\n",
    "            test_f1s.append(test_recall)\n",
    "            \n",
    "    test_epoch_loss.append(np.array(test_losses).mean())\n",
    "    test_epoch_accuracy.append(test_accuracy)\n",
    "    test_epoch_recall.append(test_recall)\n",
    "    test_epoch_precision.append(test_precision)\n",
    "    test_epoch_f1s.append(test_f1)\n",
    "    \n",
    "    plot_graphs(epoch_losses, test_epoch_loss, epoch_accuracy, test_epoch_accuracy, epoch_precision, \n",
    "         test_epoch_precision, epoch_recall, test_epoch_recall, epoch_f1s, test_epoch_f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_out_permute = ner_out.permute(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_true = data['y_ner_padded'].permute(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model(ner_out_permute.to('cpu'), ner_true.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_model.decode(ner_out_permute.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(ner_out_permute, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
